{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "768ee653",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "871438e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_params(K, D, sigma_sq_omega):\n",
    "  # Set seed so we always get the same random numbers\n",
    "  np.random.seed(0)\n",
    "\n",
    "  # Input layer\n",
    "  D_i = 1\n",
    "  # Output layer\n",
    "  D_o = 1\n",
    "\n",
    "  # Make empty lists\n",
    "  all_weights = [None] * (K+1)\n",
    "  all_biases = [None] * (K+1)\n",
    "\n",
    "  # Create input and output layers\n",
    "  all_weights[0] = np.random.normal(size=(D, D_i))*np.sqrt(sigma_sq_omega)\n",
    "  all_weights[-1] = np.random.normal(size=(D_o, D)) * np.sqrt(sigma_sq_omega)\n",
    "  all_biases[0] = np.zeros((D,1))\n",
    "  all_biases[-1]= np.zeros((D_o,1))\n",
    "\n",
    "  # Create intermediate layers\n",
    "  for layer in range(1,K):\n",
    "    all_weights[layer] = np.random.normal(size=(D,D))*np.sqrt(sigma_sq_omega)\n",
    "    all_biases[layer] = np.zeros((D,1))\n",
    "\n",
    "  return all_weights, all_biases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6c1f6271",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the Rectified Linear Unit (ReLU) function\n",
    "def ReLU(preactivation):\n",
    "  activation = preactivation.clip(0.0)\n",
    "  return activation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c598ae6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_network_output(net_input, all_weights, all_biases):\n",
    "\n",
    "  # Retrieve number of layers\n",
    "  K = len(all_weights)-1\n",
    "\n",
    "  # We'll store the pre-activations at each layer in a list \"all_f\"\n",
    "  # and the activations in a second list \"all_h\".\n",
    "  all_f = [None] * (K+1)\n",
    "  all_h = [None] * (K+1)\n",
    "\n",
    "  #For convenience, we'll set\n",
    "  # all_h[0] to be the input, and all_f[K] will be the output\n",
    "  all_h[0] = net_input\n",
    "\n",
    "  # Run through the layers, calculating all_f[0...K-1] and all_h[1...K]\n",
    "  for layer in range(K):\n",
    "      # Update preactivations and activations at this layer according to eqn 7.5\n",
    "      all_f[layer] = all_biases[layer] + np.matmul(all_weights[layer], all_h[layer])\n",
    "      all_h[layer+1] = ReLU(all_f[layer])\n",
    "\n",
    "  # Compute the output from the last hidden layer\n",
    "  all_f[K] = all_biases[K] + np.matmul(all_weights[K], all_h[K])\n",
    "\n",
    "  # Retrieve the output\n",
    "  net_output = all_f[K]\n",
    "\n",
    "  return net_output, all_f, all_h"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "57313996",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Layer 1, std of hidden units = 0.811\n",
      "Layer 2, std of hidden units = 1.472\n",
      "Layer 3, std of hidden units = 4.547\n",
      "Layer 4, std of hidden units = 8.896\n",
      "Layer 5, std of hidden units = 10.106\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Number of layers\n",
    "K = 5\n",
    "# Number of neurons per layer\n",
    "D = 8\n",
    "# Input layer\n",
    "D_i = 1\n",
    "# Output layer\n",
    "D_o = 1\n",
    "# Set variance of initial weights to 1\n",
    "sigma_sq_omega = 1.0\n",
    "# Initialize parameters\n",
    "all_weights, all_biases = init_params(K,D,sigma_sq_omega)\n",
    "\n",
    "n_data = 1000\n",
    "data_in = np.random.normal(size=(1,n_data))\n",
    "net_output, all_f, all_h = compute_network_output(data_in, all_weights, all_biases)\n",
    "\n",
    "for layer in range(1,K+1):\n",
    "  print(\"Layer %d, std of hidden units = %3.3f\"%(layer, np.std(all_h[layer])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c6547d79",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Layer 1, std of hidden units = 0.622\n",
      "Layer 2, std of hidden units = 3.108\n",
      "Layer 3, std of hidden units = 21.075\n",
      "Layer 4, std of hidden units = 161.638\n",
      "Layer 5, std of hidden units = 1125.582\n",
      "Layer 6, std of hidden units = 6319.072\n",
      "Layer 7, std of hidden units = 37275.665\n",
      "Layer 8, std of hidden units = 243387.814\n",
      "Layer 9, std of hidden units = 1339835.231\n",
      "Layer 10, std of hidden units = 7366234.399\n",
      "Layer 11, std of hidden units = 49006173.785\n",
      "Layer 12, std of hidden units = 272845366.658\n",
      "Layer 13, std of hidden units = 1682043584.115\n",
      "Layer 14, std of hidden units = 10666632256.715\n",
      "Layer 15, std of hidden units = 66098343304.232\n",
      "Layer 16, std of hidden units = 429669007251.536\n",
      "Layer 17, std of hidden units = 2889209957356.916\n",
      "Layer 18, std of hidden units = 19621779417283.500\n",
      "Layer 19, std of hidden units = 121787762396578.984\n",
      "Layer 20, std of hidden units = 999886829483868.875\n",
      "Layer 21, std of hidden units = 5334411928004678.000\n",
      "Layer 22, std of hidden units = 33827620837739412.000\n",
      "Layer 23, std of hidden units = 225444894681278176.000\n",
      "Layer 24, std of hidden units = 1627820610460267776.000\n",
      "Layer 25, std of hidden units = 11267764649765797888.000\n",
      "Layer 26, std of hidden units = 67624804921841565696.000\n",
      "Layer 27, std of hidden units = 364972590784171016192.000\n",
      "Layer 28, std of hidden units = 2240665662851632070656.000\n",
      "Layer 29, std of hidden units = 17591855121671590510592.000\n",
      "Layer 30, std of hidden units = 127571735900692099891200.000\n",
      "Layer 31, std of hidden units = 673191222755367401291776.000\n",
      "Layer 32, std of hidden units = 3013965906952738308096000.000\n",
      "Layer 33, std of hidden units = 18058080219374338489450496.000\n",
      "Layer 34, std of hidden units = 113844343088523883136942080.000\n",
      "Layer 35, std of hidden units = 743768651021983614777688064.000\n",
      "Layer 36, std of hidden units = 4212183983909334019905421312.000\n",
      "Layer 37, std of hidden units = 29165435896872114952769372160.000\n",
      "Layer 38, std of hidden units = 158335947919997755737983942656.000\n",
      "Layer 39, std of hidden units = 997715185910698111272413560832.000\n",
      "Layer 40, std of hidden units = 6426783972194131163949876379648.000\n",
      "Layer 41, std of hidden units = 43768309206637147921539531800576.000\n",
      "Layer 42, std of hidden units = 303487129329508587245138174017536.000\n",
      "Layer 43, std of hidden units = 1776138313453572474864744603320320.000\n",
      "Layer 44, std of hidden units = 11908465022449954960409252809670656.000\n",
      "Layer 45, std of hidden units = 89231570537681377058251182172012544.000\n",
      "Layer 46, std of hidden units = 537142295281509613087075230529093632.000\n",
      "Layer 47, std of hidden units = 3598219068834594188168289796459855872.000\n",
      "Layer 48, std of hidden units = 19195733181303615269323687454815813632.000\n",
      "Layer 49, std of hidden units = 112209168636519545406961012606813339648.000\n",
      "Layer 50, std of hidden units = 753218502856424628936041819469258948608.000\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Number of layers\n",
    "K = 50\n",
    "# Number of neurons per layer\n",
    "D = 80\n",
    "# Input layer\n",
    "D_i = 1\n",
    "# Output layer\n",
    "D_o = 1\n",
    "# Set variance of initial weights to 1\n",
    "sigma_sq_omega = 1.0\n",
    "# Initialize parameters\n",
    "all_weights, all_biases = init_params(K,D,sigma_sq_omega)\n",
    "\n",
    "n_data = 1000\n",
    "data_in = np.random.normal(size=(1,n_data))\n",
    "net_output, all_f, all_h = compute_network_output(data_in, all_weights, all_biases)\n",
    "\n",
    "for layer in range(1,K+1):\n",
    "  print(\"Layer %d, std of hidden units = %3.3f\"%(layer, np.std(all_h[layer])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "0e341a7c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Layer 1, std of hidden units = 0.098\n",
      "Layer 2, std of hidden units = 0.078\n",
      "Layer 3, std of hidden units = 0.083\n",
      "Layer 4, std of hidden units = 0.101\n",
      "Layer 5, std of hidden units = 0.111\n",
      "Layer 6, std of hidden units = 0.099\n",
      "Layer 7, std of hidden units = 0.092\n",
      "Layer 8, std of hidden units = 0.095\n",
      "Layer 9, std of hidden units = 0.083\n",
      "Layer 10, std of hidden units = 0.072\n",
      "Layer 11, std of hidden units = 0.076\n",
      "Layer 12, std of hidden units = 0.067\n",
      "Layer 13, std of hidden units = 0.065\n",
      "Layer 14, std of hidden units = 0.065\n",
      "Layer 15, std of hidden units = 0.064\n",
      "Layer 16, std of hidden units = 0.066\n",
      "Layer 17, std of hidden units = 0.070\n",
      "Layer 18, std of hidden units = 0.075\n",
      "Layer 19, std of hidden units = 0.073\n",
      "Layer 20, std of hidden units = 0.095\n",
      "Layer 21, std of hidden units = 0.080\n",
      "Layer 22, std of hidden units = 0.081\n",
      "Layer 23, std of hidden units = 0.085\n",
      "Layer 24, std of hidden units = 0.097\n",
      "Layer 25, std of hidden units = 0.106\n",
      "Layer 26, std of hidden units = 0.101\n",
      "Layer 27, std of hidden units = 0.086\n",
      "Layer 28, std of hidden units = 0.083\n",
      "Layer 29, std of hidden units = 0.104\n",
      "Layer 30, std of hidden units = 0.119\n",
      "Layer 31, std of hidden units = 0.099\n",
      "Layer 32, std of hidden units = 0.070\n",
      "Layer 33, std of hidden units = 0.066\n",
      "Layer 34, std of hidden units = 0.066\n",
      "Layer 35, std of hidden units = 0.068\n",
      "Layer 36, std of hidden units = 0.061\n",
      "Layer 37, std of hidden units = 0.067\n",
      "Layer 38, std of hidden units = 0.058\n",
      "Layer 39, std of hidden units = 0.057\n",
      "Layer 40, std of hidden units = 0.058\n",
      "Layer 41, std of hidden units = 0.063\n",
      "Layer 42, std of hidden units = 0.069\n",
      "Layer 43, std of hidden units = 0.064\n",
      "Layer 44, std of hidden units = 0.068\n",
      "Layer 45, std of hidden units = 0.080\n",
      "Layer 46, std of hidden units = 0.076\n",
      "Layer 47, std of hidden units = 0.081\n",
      "Layer 48, std of hidden units = 0.068\n",
      "Layer 49, std of hidden units = 0.063\n",
      "Layer 50, std of hidden units = 0.067\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Number of layers\n",
    "K = 50\n",
    "# Number of neurons per layer\n",
    "D = 80\n",
    "# Input layer\n",
    "D_i = 1\n",
    "# Output layer\n",
    "D_o = 1\n",
    "# Set variance of initial weights to 1\n",
    "sigma_sq_omega = 2 / 80\n",
    "# Initialize parameters\n",
    "all_weights, all_biases = init_params(K,D,sigma_sq_omega)\n",
    "\n",
    "n_data = 1000\n",
    "data_in = np.random.normal(size=(1,n_data))\n",
    "net_output, all_f, all_h = compute_network_output(data_in, all_weights, all_biases)\n",
    "\n",
    "for layer in range(1,K+1):\n",
    "  print(\"Layer %d, std of hidden units = %3.3f\"%(layer, np.std(all_h[layer])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "d7d3a8ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def least_squares_loss(net_output, y):\n",
    "  return np.sum((net_output-y) * (net_output-y))\n",
    "\n",
    "def d_loss_d_output(net_output, y):\n",
    "    return 2*(net_output -y);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "98dcfade",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Layer 49, std of dl_dh = 18817849764518896023941902245604242751488.000\n",
      "Layer 48, std of dl_dh = 140118487856732239819583266458026790354944.000\n",
      "Layer 47, std of dl_dh = 966020560711593611954353793080976121790464.000\n",
      "Layer 46, std of dl_dh = 5987633018620223252680859241244423494828032.000\n",
      "Layer 45, std of dl_dh = 39500717745074512367426775088265226423894016.000\n",
      "Layer 44, std of dl_dh = 231495044789922184959983952343310884879204352.000\n",
      "Layer 43, std of dl_dh = 1336239794865748360889709863959013995287937024.000\n",
      "Layer 42, std of dl_dh = 7712581729931044545150107431487218057337634816.000\n",
      "Layer 41, std of dl_dh = 40450451143651953145051042083546508464165486592.000\n",
      "Layer 40, std of dl_dh = 258241445087590776207952401654968831493980291072.000\n",
      "Layer 39, std of dl_dh = 1461454311950454565864275500814865356181534146560.000\n",
      "Layer 38, std of dl_dh = 10332488588890049713609937944320962320259371499520.000\n",
      "Layer 37, std of dl_dh = 67762952239536214087718500711727746678730564567040.000\n",
      "Layer 36, std of dl_dh = 486644995502083127708682788488469787451507406274560.000\n",
      "Layer 35, std of dl_dh = 3038521358601098311638948500818773455982912979599360.000\n",
      "Layer 34, std of dl_dh = 20622026877096516274168395047672140496941939104415744.000\n",
      "Layer 33, std of dl_dh = 115846381203838787394170905633539747898124137758982144.000\n",
      "Layer 32, std of dl_dh = 692775722996451558577043031371608601967229693599940608.000\n",
      "Layer 31, std of dl_dh = 4533651840178385141827214279520261185924243910977126400.000\n",
      "Layer 30, std of dl_dh = 27962097471983042838261328844134491112183619548352610304.000\n",
      "Layer 29, std of dl_dh = 163553513870808583520322854044457808217847000530386157568.000\n",
      "Layer 28, std of dl_dh = 956779985561020314898718891153874492925720775421178413056.000\n",
      "Layer 27, std of dl_dh = 5462460278894732756043821589879135511995149021884442476544.000\n",
      "Layer 26, std of dl_dh = 41712815717105573157388729547722781775721566276548869226496.000\n",
      "Layer 25, std of dl_dh = 189303733379321843250662423860770179803378133771958234382336.000\n",
      "Layer 24, std of dl_dh = 1167175082942496437577952826235058182828232913615180278005760.000\n",
      "Layer 23, std of dl_dh = 8400447047412217384411313403421642231123427347819587759505408.000\n",
      "Layer 22, std of dl_dh = 51359326687770714591080421119603752588039560844573823207473152.000\n",
      "Layer 21, std of dl_dh = 366590511153710368970060455455171193527375669464669979272019968.000\n",
      "Layer 20, std of dl_dh = 2521931090077676895426526752760019266445197562630231172451401728.000\n",
      "Layer 19, std of dl_dh = 12981649213666816932054674077875315121284288221707835534187954176.000\n",
      "Layer 18, std of dl_dh = 91625806080021478342291089672448001142720706841389642832077127680.000\n",
      "Layer 17, std of dl_dh = 571601369784794989446667420209202936940222209745048856247803052032.000\n",
      "Layer 16, std of dl_dh = 4228152175740395336998107534221620678971874246052051926244533469184.000\n",
      "Layer 15, std of dl_dh = 29375585020824613835255558404699003509550170184245690282892605784064.000\n",
      "Layer 14, std of dl_dh = 177343205970746394858263745701515570296774305686007880999959389011968.000\n",
      "Layer 13, std of dl_dh = 1079960630998983632876051864243704136104502226835904912467119264759808.000\n",
      "Layer 12, std of dl_dh = 7477794785556514094366085390403081262537370327604628917468459249434624.000\n",
      "Layer 11, std of dl_dh = 47040925004825722352541677340006022320097780899999487454402193947361280.000\n",
      "Layer 10, std of dl_dh = 291677266860870584899804499315518845642829095127549746074156774646087680.000\n",
      "Layer 9, std of dl_dh = 1622769590787357809644286119303254647392100482625378398972553713753784320.000\n",
      "Layer 8, std of dl_dh = 10229045579369878629168594432237202474061585903562538767196464137472311296.000\n",
      "Layer 7, std of dl_dh = 72289425689022408803366864048857004309639326011550701107824493383028572160.000\n",
      "Layer 6, std of dl_dh = 437115574218245344458350487179115097134296090284775161101148264674829008896.000\n",
      "Layer 5, std of dl_dh = 2657382923492156040986765681267977528171519851532885108585693309444049338368.000\n",
      "Layer 4, std of dl_dh = 18012406883088293788696713318573057230601444707387591493180304542869921726464.000\n",
      "Layer 3, std of dl_dh = 116256578506420975241043544836870687004014253364803063264811558279725340164096.000\n",
      "Layer 2, std of dl_dh = 644297462740804919130651565177435903179397812401254220414005154631900433743872.000\n",
      "Layer 1, std of dl_dh = 3864161615668245204213546170976925500538765385060879859456227035266840600248320.000\n"
     ]
    }
   ],
   "source": [
    "# Number of layers\n",
    "K = 50\n",
    "# Number of neurons per layer\n",
    "D = 80\n",
    "# Input layer\n",
    "D_i = 1\n",
    "# Output layer\n",
    "D_o = 1\n",
    "# Set variance of initial weights to 1\n",
    "sigma_sq_omega = 1.0\n",
    "# Initialize parameters\n",
    "all_weights, all_biases = init_params(K,D,sigma_sq_omega)\n",
    "\n",
    "# For simplicity we'll just consider the gradients of the weights and biases between the first and last hidden layer\n",
    "n_data = 100\n",
    "aggregate_dl_df = [None] * (K+1)\n",
    "for layer in range(1,K):\n",
    "  # These 3D arrays will store the gradients for every data point\n",
    "  aggregate_dl_df[layer] = np.zeros((D,n_data))\n",
    "\n",
    "\n",
    "# We'll have to compute the derivatives of the parameters for each data point separately\n",
    "for c_data in range(n_data):\n",
    "  data_in = np.random.normal(size=(1,1))\n",
    "  y = np.zeros((1,1))\n",
    "  net_output, all_f, all_h = compute_network_output(data_in, all_weights, all_biases)\n",
    "  all_dl_dweights, all_dl_dbiases, all_dl_dh, all_dl_df = backward_pass(all_weights, all_biases, all_f, all_h, y)\n",
    "  for layer in range(1,K):\n",
    "    aggregate_dl_df[layer][:,c_data] = np.squeeze(all_dl_df[layer])\n",
    "\n",
    "for layer in reversed(range(1,K)):\n",
    "  print(\"Layer %d, std of dl_dh = %3.3f\"%(layer, np.std(aggregate_dl_df[layer].ravel())))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ac2fb47",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
